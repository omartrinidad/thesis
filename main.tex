\documentclass[xcolor={x11names}]{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{booktabs}

\usepackage{tikz}
    \usetikzlibrary{shapes.geometric, backgrounds, calc}
    \usetikzlibrary{arrows}
    \usetikzlibrary{arrows.meta}
    \usetikzlibrary{positioning}
    \usetikzlibrary{chains}

\newcommand{\K}{\mathcal{K}}
\newcommand{\KB}{KB $\mathcal{K}$}

% Begin tikz section %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tikz_additions.tex}
% End tikz section %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
}

\title[Mining Logical Rules]{Mining Logical Rules}
\author{Omar Gutiérrez}
\institute{}
\date{\today}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Introduction}
\subsection{Knowledge bases}

\begin{frame}{Knowledge bases}
\begin{itemize}
  \item A knowledge base (KB) is filled with facts.
  		\begin{itemize}

        \item Every fact is represented by a \textbf{relation} between a
            \textit{subject} and \textit{object}.
        \item For example: \textit{Homer} \textbf{isHusbandOf} \textit{Marge}.
  		\end{itemize}
  \item Some popular KBs are \textbf{DBpedia}, \textbf{YAGO}, \textbf{Wikidata}, \textbf{Freebase}, etc.
\end{itemize}

\end{frame}

\begin{frame}{How they are designed?}
\begin{itemize}
  \item The Resource Description Framework (RDF) is the most popular format for the semantic KBs.
  \item Every fact in KBs is known as triple.
\end{itemize}

\vskip 0.2cm

\begin{figure}
\resizebox{7.555cm}{!}{%
    \input{diagrams/rdf.tex}
}
\caption{Very simple example of RDF graph}
\label{fig:rdf}
\end{figure}

\end{frame}

\begin{frame}{Open and Closed World Assumption}
\subsection{Open and Closed World Assumption}

\begin{itemize}
    \item \textbf{Open World Assumption} (OWA) is assumed in relational databases.
    \item \textbf{Closed World Assumption} (CWA) is assumed in semantic knowledge bases.
        \begin{itemize}
            \item Semantic KBs do not contain negative evidence :(
        \end{itemize}
    %\item \textbf{PARTIAL CLOSED World Assumption}.
\end{itemize}

% ToDo: Add color, symbols, bold
\begin{tabular}{ l | c | c }
    \toprule
    & OWA & CWA \\
    \midrule
    Counter-examples & No & Yes \\
    \bottomrule
\end{tabular}
\end{frame}

\subsection{Mining facts}
\begin{frame}{Mining facts}
\begin{itemize}
   \item Let’s say that we know the next facts
   \begin{itemize}
   		\item \textless Homer\textgreater\  \textbf{isHusbandOf} \textless Marge\textgreater\
        \item \textless Homer\textgreater\  \textbf{wasBornIn} \textless United States\textgreater\
        \item \ldots
        \item \textless Marge\textgreater\  \textbf{wasBornIn} \textless?\textgreater
   \end{itemize}
   \item Another example:
   \begin{itemize}
   		\item \textless Bart\textgreater\  \textbf{isSonOf} \textless Homer\textgreater
        \item \textless Lisa\textgreater\  \textbf{isDaughterOf} \textless Homer\textgreater
        \item \ldots
        \item \textless Bart\textgreater\  \textbf{isBrotherOf} \textless?\textgreater
   \end{itemize}
\end{itemize}
\end{frame}

%\begin{frame}{Language bias}
%    \begin{itemize}
%        \item Language bias are constraints whose aim is \textit{limit the size of the search space}.
%    \end{itemize}
%\end{frame}

\subsection{Confidence and support}

\begin{frame}{Standard Confidence}
\begin{equation}
    \label{eq:stand_conf}
    conf(\vec{B} \implies r(x, y)) =
             \dfrac{supp(\vec{B} \implies r(x, y))}
                   {\#(x, y): \exists z_1,\ldots,z_m : \vec{B}} %\,,
\end{equation}
\end{frame}

\begin{frame}{PCA Confidence}
\begin{equation}
    \label{eq:pca_conf}
    conf_{pca}(\vec{B} \implies r(x, y)) =
                    \dfrac{supp(\vec{B} \implies r(x, y))}
                          {\#(x, y): \exists z_1,\ldots,z_m, y' :\vec{B} \land r(x, y')} \,,
\end{equation}
\end{frame}

\subsection{Confidence evaluation}
\begin{frame}{Confidence evaluation}
    \begin{itemize}
       \item 
    \end{itemize}
\end{frame}

\subsection{Rule refinement}
\begin{frame}{Rule refinement}
    \begin{itemize}
       \item 
    \end{itemize}
\end{frame}

%\begin{frame}{Basic idea of the Mining Process}
%	\begin{itemize}
%        %\item Association Rule Mining under Incomplete Evidence (AMIE) is one
%        %    \textit{of several} algorithms used to mine logic rules in KB.
%        \item The idea is similar to the association rule learning.
%        % \item \textbf{Let’s take a look on it}.
%        \item Use language bias (or constraints) to reduce the size of the search space.
%            \begin{itemize}
%                \item We ignore \textit{unconnected} rules:
%                    \begin{itemize}
%                        \item \textless Homer\textgreater\  \textbf{worksOn} \textless Nuclear Plant\textgreater
%                        \item \textless Moe\textgreater\  \textbf{worksOn} \textless Moe's Pub\textgreater
%                    \end{itemize}
%                    \item We ignore \textit{reflexive} rules:
%                    \begin{itemize}
%                        \item \textless Maggie\textgreater\  \textbf{isEqualTo} \textless Maggie\textgreater
%                    \end{itemize}
%            \end{itemize}
%	\end{itemize}
%\end{frame}


\section{AMIE}

\begin{frame}{AMIE algorithm}
	\begin{itemize}
   		\item Association Rule Mining under Incomplete Evidence (AMIE) is one 
            \textit{of several} algorithms used to mine logic rules in KBs.
	    \begin{itemize}
            \item It is a similar idea to the association rule learning.
        \end{itemize}
        \item \textbf{Let’s take a look on it}.
	\end{itemize}
\end{frame}

\begin{frame}{AMIE}
\begin{columns}

\begin{column}{0.4\textwidth}
	\begin{itemize}
        \item Initialize $q$ and $out$.
        \item While the queue of rules $q$ is not empty\ldots
        \item We dequeue $q$
	\end{itemize}
\end{column}

\begin{column}{0.6\textwidth}
    \resizebox{!}{0.8\textwidth}{%
        \input{diagrams/amie_one.tex}
    }
\end{column}
\end{columns}
\end{frame}


\begin{frame}{AMIE}
\begin{columns}

\begin{column}{0.4\textwidth}
	\begin{itemize}
        \item If the rule $r$ meets the criteria is added to $out$
	\end{itemize}
\end{column}

\begin{column}{0.6\textwidth}
    \resizebox{!}{0.8\textwidth}{%
        \input{diagrams/amie_two.tex}
    }
\end{column}
\end{columns}
\end{frame}

\subsection{In-memory database and queries}

\begin{frame}{In-memory database and queries}
\begin{figure}
    \input{diagrams/db.tex}
\caption{Diagram of the database}
\label{fig:db}
\end{figure}
\end{frame}

\begin{frame}{Size queries}
\end{frame}

\begin{frame}{Count queries}
    \begin{figure}
    \resizebox{!}{0.75\textheight}{%
        \input{diagrams/count_projection.tex}
    }
    \caption{Flowchart of the count projection query algorithm}
    \label{fig:count_projection}
\end{figure}
\end{frame}

\section{AMIE+}
\begin{frame}{AMIE+: Enhancements}
	\begin{itemize}
   		\item Some stop conditions were added 
	\end{itemize}
\end{frame}

\subsection{Speeding up rule refinements}
\begin{frame}{Speeding up rule refinements}

    \begin{block}{Maximum rule length}
        We do not apply a \textit{mining operator} if the size of the generated rule is 
        larger than the \textbf{$maxLen$} threshold.
	\end{block}

    % ToDo:
    % - Find an analogy for this cases, like you do not bet if you know that you are going to lose...
    % - Make an example
	\begin{itemize}
        \item If the size of the rule is \textbf{$maxLen -1$}\ldots
	        \begin{itemize}
                \item avoid the $\mathcal{O}_\mathcal{D}$ because the results is a non-closed rule.
	        \end{itemize}
        \item If the size of the rule is \textbf{$maxLen -1$} and have more than two non-closed variables\ldots
	        \begin{itemize}
                \item avoid the $\mathcal{O}_\mathcal{C}$ because we can only close at most two variables.
                \item avoid the $\mathcal{O}_\mathcal{I}$ because of the same reason.
	        \end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}{Speeding up rule refinements}

    \begin{block}{Perfect rule}
        We stop adding atoms when the PCA confidence is 100\%
	\end{block}

    \begin{block}{Simplyfing projection queries}
        When a new dangling atom is added.
	\end{block}

\end{frame}

\begin{frame}{Speeding up the confidence evaluation}

    \begin{block}{}
        A \textbf{major change} in AMIE+ 
	\end{block}

    \begin{block}{Approximate the value of PCA confidence}
	\end{block}

    \begin{equation}
        \label{eq:approx_stand_conf}
        \widehat{conf}_{pca}(R) = \dfrac{supp(R)}{\widehat{d}_{pca}(R)}\,,
    \end{equation}

\end{frame}

\begin{frame}{When to use approximation?}
    % in the paper, intermediate values = existentially quantified variables
    \begin{block}{}
        Only when the head variables $x$ and $y$ are \textit{chained}
        in the body by intermediate variables:
	\end{block}
    % ToDo: Improve with tikz
    \begin{equation}
        \label{eq:candidate_for_approx}
        r_1(x, z_1) \land r_2(z_1, z_2) \land \ldots \land r_n(z_{n-1}, y) \implies r_h(x, y)\,,
    \end{equation}

    e.g.,

    \begin{equation}
        directed(x, y) \land hasActor(y, z) \implies isMarried(x, y)
    \end{equation}

\end{frame}

\begin{frame}{Compute the approximation}
\end{frame}

\section{Conclusions}

\begin{frame}{AMIE vs ILP}
    % ToDo: Add color, symbols, bold
    \begin{tabular}{ l | c | c }
        \toprule
        & AMIE & ILP \\
        \midrule
        Counter-examples & Works in the absense of them & Works with them \\
        Scalability      & Yes  & No\\
        \bottomrule
    \end{tabular}
\end{frame}

\begin{frame}{Conclusions}
%	\begin{itemize}
%   		\item Further work:
%	    \begin{itemize}
%   		    \item Make a Python implementation.
%	    \end{itemize}
%	\end{itemize}
\end{frame}

\end{document}
