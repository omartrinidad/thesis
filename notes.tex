\documentclass{article}

% https://tex.stackexchange.com/questions/106988/package-for-drawing-rdf-graphs
% https://tex.stackexchange.com/questions/9442/how-to-draw-lines-around-multiple-table-cells

% ALEPH
% http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph_toc.html

% WARMeR
% https://arxiv.org/abs/cs/0206023

\title{
Distributed AMIE+\\
\large
Preliminary Notes\\
of the\\
Thesis Project\\
}
\author{Omar Trinidad Guti\'errez M\'endez}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[x11names]{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{afterpage}
\usepackage{listings}
%\usepackage{venndiagram}

\newcommand{\triple}[3]{{\itshape\textless#1\textgreater} {\bfseries#2} {\itshape\textless#3\textgreater}}
\newcommand{\instantiation}[2]{{\itshape#1}{$\implies$}{\itshape#2}}
\newcommand{\ins}[2]{{#1}{\implies}{#2}}
\newcommand{\body}{\vec{B}}

% Begin tikz section %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
    \usetikzlibrary{shapes.geometric, backgrounds, calc}
    \usetikzlibrary{arrows}
    \usetikzlibrary{positioning}
    \usetikzlibrary{chains}

% Set up a few colours
\colorlet{green}{DarkOliveGreen3!50}
\colorlet{red}{IndianRed3!50}
\colorlet{blue}{DeepSkyBlue3!50}

% For every picture that defines or uses external nodes, you'll have to
% apply the 'remember picture' style. To avoid some typing, we'll apply
% the style to all pictures.
\tikzstyle{every picture}+=[remember picture]

% By default all math in TikZ nodes are set in inline mode. Change this to
% displaystyle so that we don't get small fractions.
\everymath{\displaystyle}

% Introduce a new counter for counting the nodes needed for circling
\newcounter{nodecount}
% Command for making a new node and naming it according to the nodecount counter
\newcommand\tabnode[1]{\addtocounter{nodecount}{1} \tikz \node (\arabic{nodecount}) {#1};}

% End tikz section %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\section{Introduction}

Knowledge bases (KB) have the purpose of representing and store knowledge in a
machine-readable format. These databases are populated with entities and facts.

Some well-known KBs are DBpedia~\cite{dbpedia-swj},
NELL, YAGO~\cite{suchanek2007yago}, or Freebase~\cite{bollacker2008freebase}. A
usual task executed in these databases is mining logical rules (Horn rules), that is, find
unknown relationships between entities.

Some reasons why we want to obtain logical rules are

\begin{itemize}
    \item Obtaining new facts.
    \item Identify potential new wrong facts.
    \item Use the rules for reasoning.
    \item Find especial relationships
\end{itemize}

However,  these databases are designed under the idea of Open World Assumption
(OWA), that means, if the database does not contain a fact, we are not assuming
that this fact is false, as happens under the Closed World Assumption (CWA).

Finding these relations in huge datasets, and under the OWA setting is a
challenging task. This problem was addressed by Galárraga et
al.~\cite{galarraga2013amie}  who proposed Association Rule Mining under
Incomplete Evidence (AMIE) and later suggested an improved version of the same
method that they simple named AMIE+~\cite{galarraga2015fast}.

The purpose of the current project is to explore AMIE+ and implement it in a
distributed context.

\subsection{Theoretical framework}

%~\cite{rdf}
Knowledge Bases $KB$ are collections of facts; every fact is represented by a
relation between a subject and object $r(s, o)$. In this work, we are focused
on KBs modeled using the W3C standard Resource Description Framework
(RDF). In RDF, the facts are represented as triples.

An \textit{atom} is a fact with variables at the subject and/or object. A Horn
rule is composed of a head and a body.

\begin{itemize}
    \item The head is a single atom \tikz\node [fill=green!20,draw,circle] (n3) {};
    \item The body is a set of atoms \tikz\node [fill=blue!20,draw,circle] (n1) {};
\end{itemize}

% Below we mix an ordinary equation with TikZ nodes
\begin{equation}
        \tikz[baseline]{
            \node[fill=blue!20,anchor=base] (b1)
            { $B_1$ };
        } \land
        \tikz[baseline]{
            \node[fill=blue!20,anchor=base] (b2)
            { $B_2$ };
        } \land \ldots \land
        \tikz[baseline]{
            \node[fill=blue!20,anchor=base] (b3)
            { $B_n$ };
        }
        \implies
        \tikz[baseline]{
            \node[fill=green!20,anchor=base] (head)
            { $r(x, y)$ };
        }
\end{equation}

% Draw some edges between the global nodes
\begin{tikzpicture}[overlay]
        \path[->] (n1) edge [out=-100, in=-250] (b1);
        \path[->] (n1) edge [out=-100, in=-250] (b2);
        \path[->] (n1) edge [out=-100, in=-250] (b3);
        \path[->] (n3) edge [out=0, in=-250] (head);
\end{tikzpicture}

% ¿Es relevante?
% The facts in KBs are divided in A-Box and T-Box.

\subsubsection{Incompleteness}

As was stated before, the semantic KBs operate under the CWA, that is, we
assume the facts in the database are known true facts, everything else, outside
the database is asumed to be unknown.

Going beyond, we say that the unknown facts are either true or false facts, we
want to predict the new ones.

\begin{figure}
\centering
\resizebox{7.555cm}{!}{%
    \input{diagrams/kb.tex}
}
\caption{Incompleteness}
\label{fig:kb}
\end{figure}

\subsubsection{Rule Mining}

\subsubsection{Significance: Support and Head Coverage}

We look for meaningful rules, we have two measures of \textit{rule significance}:
support and head coverage. The \textbf{support} is the number of instantiations or
correct predictions of some rule, it is defined as

\begin{equation}
    \label{eq:support}
    supp(\vec{B} \implies r(x, y)) = \#(x, y): \exists z_1,\ldots,z_m : \vec{B} \land r(x, y) \,,
\end{equation}

\noindent where $z_1, \ldots, z_m$ are the variables of the rule that are not
$x$ and $y$. Let's consider as example the rule

\begin{equation*}
    R = \ins{livesIn(x, y)}{wasBornIn(x,y)}
\end{equation*}

\noindent and the tiny knowledge base presented in~\ref{table:significance}; we can see that
the rule is instantiated only once, \instantiation{livesIn(Bart, Springfield)}{wasBornIn(Bart, Springfield)}.

\begin{table}[ht]
    \centering
    \begin{tabular}{ c c }
    \toprule
        livesIn & wasBornIn \\
    \midrule
        \tabnode{(Bart, Springfield) } & \tabnode{(Bart, Springfield)} \\
        (Willy, Springfield)           & (Apu, India)                  \\
        (Homer, Springfield)           & \tabnode{(Willy, Scotland)}   \\
    \bottomrule
    \end{tabular}
    \caption{Tiny knowlege base}
    \label{table:significance}
\end{table}

A better way to measure the significance, is using the \textbf{head coverage}, defined as

\begin{equation}
    \label{eq:hc}
    hc(\vec{B} \implies r(x, y)) = \dfrac{supp(\vec{B} \implies r(x, y))}{size(r)} \,,
\end{equation}

\noindent where $size(r)$ is the number of times that the head rule it does
appear in the knowledge base, in the example above, \textit{wasBornIn} is
repeated two times, hence, $hc(R) = 1/3$.

\subsubsection{Confidence: Standard confidence and PCA confidence}

\begin{table}[]
    \centering
    \begin{tabular}{ c c }
    \toprule
        livesIn & wasBornIn \\
    \midrule
        \tabnode{(Bart, Springfield)}  & \tabnode{(Bart, Springfield)} \\
        \tabnode{(Willy, Springfield)} & (Apu, India)                  \\
                 (Homer, Springfield)  & (Willy, Scotland)             \\
        \tabnode{(Bart, Shellbyville)} &                               \\
    \bottomrule
    \end{tabular}

    \caption{
        Framed in red, is the entire set of conclusions that comply
        with the specified body $\body$; framed in blue, is the fact
        considered to be true; in green, all the remaining facts, negative in the CWA and
        unknown in the OWA. In brown, the facts negative facts in the PCA.
    }

    \label{table:confidence}
\end{table}

\begin{tikzpicture}[overlay]
    % some options common to all the nodes and paths
    \tikzstyle{every picture}+=[remember picture,baseline]
    \tikzstyle{every node}+=[inner sep=0pt,anchor=base,
    minimum width=1.8cm,align=center,text depth=.25ex,outer sep=1.5pt]
    \tikzstyle{every path}+=[thick, rounded corners]

    % Define the circle paths
    \draw [blue] (1.north west) -- (2.north east) --
                 (2.south east) -- (1.south west) --
                 cycle;
    \draw [red] (2.north west) -- (2.north east) --
                (3.south east) -- (3.south west) --
                cycle;
    \draw [blue] (4.north west) -- (5.north east) --
                 (5.south east) -- (4.south west) --
                 cycle;
    \draw [red] (4.north west) -- (4.north east) --
                (7.south east) -- (7.south west) --
                cycle;
    \draw [green] (6.north west) -- (6.north east) --
                  (7.south east) -- (7.south west) --
                  cycle;
    %labels
    \node [right=2.333cm, minimum width=1pt] at (2) (A) {$supp$};
    \draw [<-, out=5, in=180] (2) to (A);
    \node [right=2.333cm, minimum width=1pt] at (3) (A) {$size(r)$};
    \draw [<-, out=5, in=180] (3) to (A);
    \node [right=2.333cm, minimum width=1pt] at (5) (C) {$size(r)$};
    \draw [<-, out=5, in=180] (5) to (C);
\end{tikzpicture}

The confidence measures are used to determine the quality of a rule. The
\textbf{standard confidence} defined as 

\begin{equation}
    \label{eq:stand_conf}
    conf(\vec{B} \implies r(x, y)) = 
             \dfrac{supp(\vec{B} \implies r(x, y))}
                   {\#(x, y): \exists z_1,\ldots,z_m : \vec{B}} %\,,
\end{equation}

\noindent is usually used in association rule mining in the context of market basket
analysis and suppose a CWA universe, this setting is different to the one we are dealing with, but is a good reference. In AMIE, was proposed the \textbf{PCA
confidence}, to deal with the rule mining problem in an OWA universe. 

\begin{equation}
    \label{eq:stand_conf}
    conf_{pca}(\vec{B} \implies r(x, y)) = 
                    \dfrac{supp(\vec{B} \implies r(x, y))}
                          {\#(x, y): \exists z_1,\ldots,z_m, y' :\vec{B} \land r(x, y')} \,,
\end{equation}

%
% Improve this part!!!
%
In the PCA confidence, we assume that knowing one $y$ for a given $x$ and $r$,
we know all $y$ for that $x$ and $r$.

In the shown example, we look at the body of $R$; the relation
\textit{livesIn}, it does appear four times, and the rule is instantiated
once, the corresponding fact in the instantiated rule is considered as true. If we were considering the OWA, the three remaining facts
would be marked as unknown, the same facts would be marked as false in the CWA.

In the PCA assumption, the main purpose is to identify counter-examples from
the set of unknown facts, in the example, we see that the fact
\textit{livesIn(Bart, Shelbyville)} is contradicting the fact
\textit{livesIn(Bart, Springfield)}, which is known to be true, hence, we
consider this fact as false.

\subsubsection{Function, functionality, and inverse functionality}

The relations with \textit{at most one object} for every subject are called
\textit{functions}, for example,

\begin{itemize}
    \item \triple{Bart}{hasBirtdate}{1980} or
    \item \triple{Lisa}{hasBiologicalMother}{Marge}.
\end{itemize}

There are other type of relations, which properly, are not functions, but are
alike functions, for example,

\begin{itemize}
    \item \triple{Bart}{hasNationality}{USA} or
    \item \triple{Jurassic Park}{wasDirectedBy}{Steven Spielberg}.
\end{itemize}

Most of the movies have only one director and most of the people have one
nationality, so, the behavior of these relations is \textit{almost like a
function}. But, there are exceptions to the rule, e. g.,

\begin{itemize}
    \item \triple{Jim Carrey}{hasNationality}{Canada} and
    \item \triple{Jim Carrey}{hasNationality}{USA}. Or
    \item \triple{Poltergeist}{wasDirectedBy}{Tobe Hooper} and
    \item \triple{Poltergeist}{wasDirectedBy}{Steven Spielberg}.
\end{itemize}

So, these relations are not functions. Nevertheless, we say that have a high
degree of \textit{functionality}, which is expressed with the next equation:

\begin{equation} %\label{eq:functionality}
    fun(r) = \dfrac{\#x : \exists y : r(x, y)}{\#(x, y) : r(x, y)} \,.
\end{equation}

\noindent Another related concept is \textit{inverse functionality} $ifun(r) =
fun(r^{-1})$, in the inverse functions, every object have at most one subject,
e. g.,

\begin{itemize}
    \item \triple{Marge}{hasDaughter}{Lisa}, here, \textbf{hasDaughter}$^{-1}$ = \textbf{hasMother}.
\end{itemize}

In AMIE, it is assumed that functionality is usually higher than the inverse
functionality.

%\begin{equation} \label{eq:functionality}
%    fun(r)
%\end{equation}

\subsubsection{Language bias}

In order to limit the size of the search space, AMIE uses constraints that are called
\textit{language bias}.

% ToDo
% Tikz figure showing examples of rules.

There is an aim to limit the size of the search space, with AMIE, for example,
we use constraints on the structure of the mined rules, this is called language
bias. The idea is to have good designed language bias to avoid to deal with an
intractable search space but at the same time to generate more expressive rules.

\begin{itemize}
    \item We aim for connectivity, two atoms in a rule are connected if they share a variable or
entity. A rule is connected when every atom is connected transitively to the
rest of atoms.
    \item The rules have to be closed.
    \item Also, reflexive rules are discarded.
\end{itemize}

\subsection{Similar works}

The task of finding new logical rules given a KB has been addressed from
multiple angles. For example, ILP based approaches, relational machine learning
or hybrid approaches.

One advantage, from AMIE over relational machine learning approaches, is that
AMIE has better interpretability, which is a crucial in the Data Science world.
So, with AMIE, it is possible to mine logical rules where there is a
correlation in the data.

\section{AMIE+ algorithm}

% The purpose of AMIE is to generate Horn rules.
%\subsubsection{Completeness}
%Lets say that $KB*$ contains every fact of the world.

As mentioned before, AMIE+ uses the Partial Completeness Assumption (PCA) to
guess the counter-examples, it also uses pruning strategies and approximations
that allow exploring the search space more efficiently. 

As seen in the Figure~\ref{fig:amie_flowchart}, AMIE+ receives four parameters,
the knowledge base KB $\mathcal{K}$, the minimum head coverage $minHC$, the
maximum size of the rules $maxLen$, and the threshold in the confidence
$minConf$. We initialize a queue $q$ with all the head atoms $r(x,y)$ and a
list $out$ to save the result (blue).

We proceed to iterate and dequeue $q$, firstly we evaluate if the rule is accepted for
output (see~\ref{ssec:filter_output}),

% consider this!
% https://blogs.gnome.org/muelli/2011/04/perfectly-scale-an-image-to-the-rest-of-a-page-with-latex/
\afterpage{%
\begin{figure}[p]
\centering
    \resizebox{!}{0.90\textheight}{%
    \input{diagrams/amie_flowchart.tex}
}
\caption{Flowchart of AMIE algorithm}
\label{fig:amie_flowchart}
\end{figure}
\clearpage
}

\subsection{Refinement, rule expansion, mining operators}

% traverse = crisscross

The combination of conjunctions of atoms form a huge search space, which is
impossible to explore. In AMIE, a set of \textbf{mining operators} is used to
extend the rules iteratively and traverse the search space. 

\subsubsection{Mining operators}

The mining operators add a new atom $r(x, y)$ to some rule $B_1 \land \ldots
\land B_n$. To do that, \textbf{count projection queries} are implemented. Not
every relation $r$ is tested, are select only those above the head-coverage threshold.

% Querying with Scala and Spark

\begin{lstlisting}[
           language=SQL,
           showspaces=false,
           basicstyle=\ttfamily,
           numbers=left,
           numberstyle=\tiny,
           commentstyle=\color{gray},
           mathescape=true
        ]
SELECT $r$, COUNT($H$) 
WHERE $H \land B_1 \land \ldots \land B_n \land r(X, Y)$
SUCH THAT COUNT($H$) $\geq minHC \times size(H)$
\end{lstlisting}

\begin{itemize}
    \item $X$ and $Y$ are variables
    \item $H$ is the head
\end{itemize}
 
\noindent \textbf{Dangling atom operator} ($\mathcal{O}_\mathcal{C}$): We add a new atom with a
fresh variable $w$ and a shared variable with the rule, that is, 

\begin{equation}
    r(X, Y) \in \{ r(x, w), r(y, w), r(w, x), r(w, y) \}\,.
\end{equation}

\noindent \textbf{Closing atom operator} ($\mathcal{O}_\mathcal{D}$): We aim to close the variables
that are open.

\noindent \textbf{Instantiated atom operator} ($\mathcal{O}_\mathcal{I}$):

\begin{equation}
    r(X, Y) \in \{ r(x, w), r(y, w), r(w, x), r(w, y) \}\,.
\end{equation}

\subsection{In-memory database}
\label{ssec:in_memory_database}

The efficient implementation of count projection queries was done implementing
an in-memory database...

Having the three columns O, R, and S

\subsection{Filter the output}
\label{ssec:filter_output}

After dequeuing a rule $r$ from $q$ we filter those ones\ldots

\begin{itemize}
    \item that are not closed and not connected and whose confidence value is 
        below that the $minConf$ threshold (blue block in Fig~\ref{fig:accepted_for_output}),
    %\item that has lower confidence than the ones stored in $out$
    \item whose refinements $B_1 \land \ldots \land B_n \land B_{n+1} \implies H$ 
        have equal or lower confidence than their parent 
        $B_1 \land \ldots \land B_n \implies H$ (red blocks)
\end{itemize}

\begin{figure}[H]
\centering
\resizebox{!}{0.50\textheight}{%
    \input{diagrams/accepted_for_output.tex}
}
\caption{Diagram of the rule filtering.}
\label{fig:accepted_for_output}
\end{figure}

\subsection{Pruning strategies}

\section{SANSA Stack}
% Tikz figures for SANSA stack.

SANSA~\cite{lehmann-2017-sansa-iswc} is a platform whose purpose is\ldots

A similar work to SANSA is S2RDF~\cite{schatzle2016s2rdf}.

\bibliographystyle{plain}
\bibliography{notes}

\end{document}
